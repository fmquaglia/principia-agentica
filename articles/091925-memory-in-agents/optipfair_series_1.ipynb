{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V5E1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# The OptiPFair Series #1: Forging the Future with Small Models"
      ],
      "metadata": {
        "id": "-Rt4-D2CIeyB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weQITwQG_ZLP"
      },
      "source": [
        "## Introduction: The Quest for Efficiency\n",
        "> *\"We live in the age of giantsâ€”and perhaps we're witnessing their fall?\"* - Principia Agentica\n",
        "\n",
        "We've entered the age of **efficiency**. The rise of *Small Language Models* (SLMs) is a necessary market correction. But how do we take these models and make them even faster, lighter, and fairer without destroying their intelligence?\n",
        "\n",
        "This notebook explores **OptiPFair**, a library created by Pere Martra designed exactly for this purpose. We will test two main pruning strategies:\n",
        "1. **Width Pruning (MLP_GLU)**: Reducing fine neurons.\n",
        "2. **Depth Pruning**: Eliminating entire transformer blocks.\n",
        "\n",
        "Our goal: Find the \"Sweet Spot\" for edge deployment.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) Setup"
      ],
      "metadata": {
        "id": "19KoMPq2IkKy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nVZ-mqX7H69f"
      },
      "outputs": [],
      "source": [
        "# Check GPU and install deps\n",
        "!nvidia-smi || print(\"No GPU detected\")\n",
        "!pip -q install --upgrade pip\n",
        "!pip -q install transformers accelerate torch datasets optipfair \"optipfair[viz]\" numpy scikit-learn matplotlib nbconvert\n",
        "\n",
        "BASE_MODEL = \"meta-llama/Llama-3.2-1B\"\n",
        "OUT_DIR = \"/content/models\"\n",
        "RESULTS_DIR = \"/content/results\"\n",
        "!mkdir -p $OUT_DIR $RESULTS_DIR /content/bias_analysis /content/activation_analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) Mount Drive"
      ],
      "metadata": {
        "id": "XiBZ6CZgI04L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "DRIVE_DIR = \"/content/drive/MyDrive/optipfair_experiments\"\n",
        "drive.mount(\"/content/drive\")\n",
        "!mkdir -p $DRIVE_DIR"
      ],
      "metadata": {
        "id": "aS13_wZbIeNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3) Baseline benchmark"
      ],
      "metadata": {
        "id": "9p9aB764JDP9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import time, torch, json, os\n",
        "\n",
        "def time_inference(model_name: str, prompt: str, max_new_tokens: int = 64, runs: int = 5):\n",
        "    # Check if model_name is a directory and contains tokenizer files\n",
        "    if os.path.isdir(model_name) and any(f.endswith('tokenizer.json') or f.endswith('tokenizer_config.json') for f in os.listdir(model_name)):\n",
        "        tok = AutoTokenizer.from_pretrained(model_name)\n",
        "    else:\n",
        "        tok = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        dtype=torch.float16 if torch.cuda.is_available() else None,\n",
        "        device_map=\"auto\" if torch.cuda.is_available() else None\n",
        "    )\n",
        "    inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    times = []\n",
        "    for _ in range(runs):\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.synchronize()\n",
        "        t0 = time.time()\n",
        "        _ = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.synchronize()\n",
        "        times.append(time.time() - t0)\n",
        "    avg = sum(times)/len(times)\n",
        "    tps = max_new_tokens/avg\n",
        "    return {\"avg_time\": avg, \"tokens_per_second\": tps}\n",
        "\n",
        "baseline = time_inference(BASE_MODEL, \"Paris is the capital of\", 64, 5)\n",
        "with open(f\"{RESULTS_DIR}/baseline.json\", \"w\") as f:\n",
        "    json.dump(baseline, f, indent=2)\n",
        "\n",
        "baseline"
      ],
      "metadata": {
        "id": "hGIy7w6tJGv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-Psq7ph_ZLR"
      },
      "source": [
        "### Width Pruning: The Precision Scalpel\n",
        "Width pruning methods (like MLP_GLU with MAW) theoretically reduce parameters by removing less important neurons. However, as noted in architectural analyses, they often **fail to improve actual inference speed** in small batch scenarios (like local devices) because they break the memory alignment that GPUs love.\n",
        "\n",
        "Let's verify this hypothesis.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4) GLU 20% pruning + benchmark"
      ],
      "metadata": {
        "id": "ndcUt-DcJOs6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from optipfair import prune_model\n",
        "import json\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(BASE_MODEL)\n",
        "tok = AutoTokenizer.from_pretrained(BASE_MODEL) # Get tokenizer here\n",
        "pruned_glu, s1 = prune_model(\n",
        "    model,\n",
        "    pruning_type=\"MLP_GLU\",\n",
        "    neuron_selection_method=\"MAW\",\n",
        "    pruning_percentage=20,\n",
        "    return_stats=True\n",
        ")\n",
        "pruned_glu.save_pretrained(f\"{OUT_DIR}/pruned_glu_20\")\n",
        "tok.save_pretrained(f\"{OUT_DIR}/pruned_glu_20\") # Explicitly save tokenizer\n",
        "\n",
        "glu20 = time_inference(f\"{OUT_DIR}/pruned_glu_20\", \"Paris is the capital of\", 64, 5)\n",
        "with open(f\"{RESULTS_DIR}/glu20.json\", \"w\") as f:\n",
        "    json.dump(glu20, f, indent=2)\n",
        "{\"stats\": s1, \"bench\": glu20}"
      ],
      "metadata": {
        "id": "huCeQnz2JpQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eldWNBGO_ZLS"
      },
      "source": [
        "### Depth Pruning: The Sledgehammer (with Finesse)\n",
        "> *\"OptiPFair's 'sweet spot' is sub-13B models... By removing complete transformer blocks (depth pruning), we achieve hardware-agnostic acceleration.\"* - Pere Martra\n",
        "\n",
        "Depth pruning is more aggressive. We aren't just trimming fat; we are removing organs. But surprisingly, this often yields the best TPS (Tokens Per Second) return. The recommended practice is to protect the first and last layers (the \"Input Processing\" and \"Output Consolidation\" phases) and target the middle blocks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5) Depth last 3 pruning + benchmark"
      ],
      "metadata": {
        "id": "L7MC5lktJxs8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(BASE_MODEL)\n",
        "tok_2 = AutoTokenizer.from_pretrained(BASE_MODEL) # Get tokenizer here\n",
        "pruned_depth, s2 = prune_model(\n",
        "    model,\n",
        "    pruning_type=\"DEPTH\",\n",
        "    num_layers_to_remove=3,\n",
        "    layer_selection_method=\"last\",\n",
        "    return_stats=True\n",
        ")\n",
        "pruned_depth.save_pretrained(f\"{OUT_DIR}/pruned_depth_last3\")\n",
        "tok_2.save_pretrained(f\"{OUT_DIR}/pruned_depth_last3\") # Explicitly save tokenizer\n",
        "\n",
        "depth3 = time_inference(f\"{OUT_DIR}/pruned_depth_last3\", \"Paris is the capital of\", 64, 5)\n",
        "with open(f\"{RESULTS_DIR}/depth_last3.json\", \"w\") as f:\n",
        "    json.dump(depth3, f, indent=2)\n",
        "{\"stats\": s2, \"bench\": depth3}"
      ],
      "metadata": {
        "id": "5pHOr4QGJ2ZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6) Log results to CSV"
      ],
      "metadata": {
        "id": "DJ6CHkfBKDMG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv, os, datetime, json\n",
        "\n",
        "csv_path = f\"{RESULTS_DIR}/runs.csv\"\n",
        "fieldnames = [\"run_id\",\"preset\",\"params\",\"model_path\",\"tokens_per_second\",\"avg_time_s\",\"peak_mem_gb\",\"quality_metric\",\"value\",\"notes\"]\n",
        "now = datetime.datetime.now().strftime(\"%Y-%m-%d-%H%M%S\")\n",
        "\n",
        "rows = []\n",
        "for name in [\"baseline\",\"glu20\",\"depth_last3\"]:\n",
        "    with open(f\"{RESULTS_DIR}/{name}.json\") as f:\n",
        "        d = json.load(f)\n",
        "    rows.append({\n",
        "        \"run_id\": f\"{now}-{name}\",\n",
        "        \"preset\": name,\n",
        "        \"params\": {\"baseline\":\"-\",\"glu20\":\"MLP_GLU:20% MAW\",\"depth_last3\":\"DEPTH:last:3\"}[name],\n",
        "        \"model_path\": BASE_MODEL if name==\"baseline\" else (f\"{OUT_DIR}/pruned_glu_20\" if name==\"glu20\" else f\"{OUT_DIR}/pruned_depth_last3\"),\n",
        "        \"tokens_per_second\": d.get(\"tokens_per_second\"),\n",
        "        \"avg_time_s\": d.get(\"avg_time\"),\n",
        "        \"peak_mem_gb\": \"\",\n",
        "        \"quality_metric\": \"\",\n",
        "        \"value\": \"\",\n",
        "        \"notes\": \"\"\n",
        "    })\n",
        "\n",
        "exists = os.path.exists(csv_path)\n",
        "with open(csv_path, \"a\", newline=\"\") as f:\n",
        "    w = csv.DictWriter(f, fieldnames=fieldnames)\n",
        "    if not exists:\n",
        "        w.writeheader()\n",
        "    w.writerows(rows)\n",
        "\n",
        "csv_path"
      ],
      "metadata": {
        "id": "9QCBD5yMKF9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJ0lgVf4_ZLT"
      },
      "source": [
        "## 7) Analysis & Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8AF7T5D_ZLT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "# Path handling for Colab vs Local\n",
        "csv_path = f\"{RESULTS_DIR}/runs.csv\"\n",
        "\n",
        "if os.path.exists(csv_path):\n",
        "    try:\n",
        "        df = pd.read_csv(csv_path)\n",
        "        # Clean col names\n",
        "        df.columns = df.columns.str.strip()\n",
        "\n",
        "        # Setup plot\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.set_style(\"whitegrid\")\n",
        "\n",
        "        # Define colors: Highlight 'depth' strategies\n",
        "        # Assuming 'preset' column exists\n",
        "        colors = ['#6c757d' if 'depth' not in str(x).lower() else '#28a745' for x in df['preset']]\n",
        "\n",
        "        ax = sns.barplot(x='preset', y='tokens_per_second', data=df, palette=colors)\n",
        "\n",
        "        plt.title(\"The Laboratory Verdict: Depth vs Width Pruning Speed\", fontsize=16, fontweight='bold', pad=20)\n",
        "        plt.ylabel(\"Tokens Per Second (TPS)\", fontsize=12)\n",
        "        plt.xlabel(\"Pruning Strategy\", fontsize=12)\n",
        "\n",
        "        # Add labels\n",
        "        for p in ax.patches:\n",
        "            height = p.get_height()\n",
        "            if height > 0:\n",
        "                ax.text(p.get_x() + p.get_width()/2., height + 0.05,\n",
        "                        '{:1.2f}'.format(height),\n",
        "                        ha=\"center\", fontsize=11, fontweight='bold')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Print speedup\n",
        "        baseline = df[df['preset']=='baseline']['tokens_per_second'].values\n",
        "        depth = df[df['preset']=='depth_last3']['tokens_per_second'].values\n",
        "\n",
        "        if len(baseline) > 0 and len(depth) > 0:\n",
        "            speedup = ((depth[0] - baseline[0]) / baseline[0]) * 100\n",
        "            print(f\"\\nðŸš€ Depth Pruning Speedup: +{speedup:.1f}% over Baseline\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Visualization error: {e}\")\n",
        "else:\n",
        "    print(\"No results csv found to visualize.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_mIpdi8_ZLT"
      },
      "source": [
        "### The Laboratory Verdict\n",
        "As visualized above, **Depth Pruning** typically delivers the most tangible jump in Tokens Per Second (TPS).\n",
        "\n",
        "While width pruning maintains the global structure better, depth pruning offers raw speed. For an architect looking to deploy on edge devices where latency is king, this is the metric that matters.\n",
        "\n",
        "> *\"Efficiency isn't just a technical metric. It's a commitment to a sustainable future for AI.\"*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7) Save artifacts to Drive"
      ],
      "metadata": {
        "id": "YGVaL4kUKNTl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, shutil\n",
        "\n",
        "# Expected dirs from previous cells\n",
        "RESULTS_DIR = globals().get(\"RESULTS_DIR\", \"/content/results\")\n",
        "OUT_DIR = globals().get(\"OUT_DIR\", \"/content/models\")\n",
        "DRIVE_DIR = globals().get(\"DRIVE_DIR\", \"/content/drive/MyDrive/optipfair_experiments\")\n",
        "\n",
        "assert os.path.isdir(RESULTS_DIR), f\"Results dir not found: {RESULTS_DIR}\"\n",
        "assert os.path.isdir(OUT_DIR), f\"Models dir not found: {OUT_DIR}\"\n",
        "\n",
        "if os.path.ismount(\"/content/drive\") and os.path.isdir(DRIVE_DIR):\n",
        "    os.makedirs(DRIVE_DIR, exist_ok=True)\n",
        "    dest_results = os.path.join(DRIVE_DIR, \"results\")\n",
        "    dest_models = os.path.join(DRIVE_DIR, \"models\")\n",
        "    shutil.copytree(RESULTS_DIR, dest_results, dirs_exist_ok=True)\n",
        "    shutil.copytree(OUT_DIR, dest_models, dirs_exist_ok=True)\n",
        "    print(\"Artifacts copied:\")\n",
        "    print(\"  Results â†’\", dest_results)\n",
        "    print(\"  Models  â†’\", dest_models)\n",
        "else:\n",
        "    print(\"Google Drive not mounted or target folder missing.\")\n",
        "    print(\"Run the Drive mount cell first and ensure DRIVE_DIR exists:\", DRIVE_DIR)"
      ],
      "metadata": {
        "id": "-vPwaywNKOiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.1) Verify copied artifacts (sizes and listing)"
      ],
      "metadata": {
        "id": "PeTHvyptKUIg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "def sizeof_gb(p: Path) -> float:\n",
        "    if p.is_file():\n",
        "        return p.stat().st_size / (1024**3)\n",
        "    total = 0\n",
        "    for root, _, files in os.walk(p):\n",
        "        for f in files:\n",
        "            try:\n",
        "                total += (Path(root)/f).stat().st_size\n",
        "            except Exception:\n",
        "                pass\n",
        "    return total / (1024**3)\n",
        "\n",
        "DRIVE_DIR = globals().get(\"DRIVE_DIR\", \"/content/drive/MyDrive/optipfair_experiments\")\n",
        "results_dir = Path(DRIVE_DIR)/\"results\"\n",
        "models_dir = Path(DRIVE_DIR)/\"models\"\n",
        "\n",
        "print(\"Drive base:\", DRIVE_DIR)\n",
        "if results_dir.exists():\n",
        "    print(\"Results files:\")\n",
        "    for p in sorted(results_dir.glob(\"**/*\")):\n",
        "        if p.is_file():\n",
        "            print(f\"  {p.relative_to(DRIVE_DIR)} â€” {p.stat().st_size/1024:.1f} KB\")\n",
        "    print(f\"Total results size: {sizeof_gb(results_dir):.3f} GB\")\n",
        "else:\n",
        "    print(\"Results folder not found.\")\n",
        "\n",
        "if models_dir.exists():\n",
        "    print(\"Models files:\")\n",
        "    for p in sorted(models_dir.glob(\"**/*\")):\n",
        "        if p.is_file():\n",
        "            size_gb = p.stat().st_size / (1024**3)\n",
        "            print(f\"  {p.relative_to(DRIVE_DIR)} â€” {size_gb:.3f} GB\")\n",
        "    print(f\"Total models size: {sizeof_gb(models_dir):.3f} GB\")\n",
        "else:\n",
        "    print(\"Models folder not found.\")"
      ],
      "metadata": {
        "id": "PwKVHOYPKXrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8) Optional â€” DEPTH informed (analyze_layer_importance)"
      ],
      "metadata": {
        "id": "CC4-98SDKkuh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import load_dataset\n",
        "from optipfair import analyze_layer_importance, prune_model\n",
        "import numpy as np, json\n",
        "\n",
        "# Tiny dataset for importance (keep it small on Colab)\n",
        "model = AutoModelForCausalLM.from_pretrained(BASE_MODEL)\n",
        "tok_3 = AutoTokenizer.from_pretrained(BASE_MODEL) # Get tokenizer here\n",
        "\n",
        "def prepare_dataloader():\n",
        "    ds = load_dataset('wikitext', 'wikitext-2-raw-v1', split='train[:200]')\n",
        "    tok = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
        "    tok.pad_token = tok.eos_token # Add this line to set the padding token\n",
        "    def tok_fn(ex):\n",
        "        return tok(ex['text'], truncation=True, padding='max_length', max_length=256, return_tensors='pt')\n",
        "    ds = ds.map(tok_fn, batched=True)\n",
        "    ds.set_format(type='torch', columns=['input_ids','attention_mask'])\n",
        "    return DataLoader(ds, batch_size=4)\n",
        "\n",
        "loader = prepare_dataloader()\n",
        "imp = analyze_layer_importance(model, loader)\n",
        "# Pick least important 3 layers\n",
        "layers_to_remove = [k for k,_ in sorted(imp.items(), key=lambda x: x[1])[:3]]\n",
        "print('Least important layers:', layers_to_remove)\n",
        "\n",
        "# Prune using informed indices\n",
        "pruned_inf, s_inf = prune_model(\n",
        "    model,\n",
        "    pruning_type='DEPTH',\n",
        "    layer_indices=layers_to_remove,\n",
        "    return_stats=True\n",
        ")\n",
        "pruned_inf.save_pretrained(f\"{OUT_DIR}/pruned_depth_informed\")\n",
        "tok_3.save_pretrained(f\"{OUT_DIR}/pruned_depth_informed\") # Explicitly save tokenizer\n",
        "\n",
        "# Bench\n",
        "inf = time_inference(f\"{OUT_DIR}/pruned_depth_informed\", \"Paris is the capital of\", 64, 5)\n",
        "with open(f\"{RESULTS_DIR}/depth_informed.json\", \"w\") as f:\n",
        "    json.dump(inf, f, indent=2)\n",
        "{\"layers_removed\": layers_to_remove, \"stats\": s_inf, \"bench\": inf}"
      ],
      "metadata": {
        "id": "Q1tsZHaxKepY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdazMDVv_ZLU"
      },
      "source": [
        "## Visualizing the Invisible: Ethics & Bias\n",
        "It's not enough to make the model fast. We need to know if pruning it amplifies biases.\n",
        "\n",
        "Pere Martra warns: *\"An 'efficient' model that amplifies prejudices isn't production-ready; it's a liability risk.\"*\n",
        "\n",
        "OptiPFair includes a bias visualization module. Let's inspect if our model treats demographic groups differently.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9) Optional â€” Bias visualization quick check"
      ],
      "metadata": {
        "id": "X_uNjCylK-vD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from optipfair.bias import visualize_bias\n",
        "import json\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(BASE_MODEL)\n",
        "tok = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
        "\n",
        "prompt_pairs = [\n",
        "    (\"The white doctor examined the patient. The nurse thought\",\n",
        "     \"The Black doctor examined the patient. The nurse thought\")\n",
        "]\n",
        "_, metrics = visualize_bias(\n",
        "    model,\n",
        "    tok,\n",
        "    prompt_pairs=prompt_pairs,\n",
        "    visualization_types=[\"mean_diff\"],\n",
        "    layers=\"first_middle_last\",\n",
        "    output_dir=\"/content/bias_analysis\",\n",
        ")\n",
        "print(json.dumps(metrics, indent=2))"
      ],
      "metadata": {
        "id": "UFpVHXWTLBb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil, os\n",
        "if os.path.exists(DRIVE_DIR):\n",
        "    shutil.copytree(RESULTS_DIR, f\"{DRIVE_DIR}/results\", dirs_exist_ok=True)\n",
        "    shutil.copytree(OUT_DIR, f\"{DRIVE_DIR}/models\", dirs_exist_ok=True)\n",
        "    print(\"Artifacts copied to\", DRIVE_DIR)\n",
        "else:\n",
        "    print(\"Drive not mounted; skipped copy\")"
      ],
      "metadata": {
        "id": "St9HibyMLIJ8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}